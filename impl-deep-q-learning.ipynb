{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09706c7",
   "metadata": {},
   "source": [
    "# Implementation Detials of Deep Q-Learning Agent\n",
    "\n",
    "I plan to use the \"CarRacing-v3\" environment from OpenAI Gym to implement and test our Deep Q-Learning algorithm. \n",
    "\n",
    "First of all, import a bunch of libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c40de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym # OpenAI Gymnasium is a game environment library.\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# provide named tuples for replay buffer in DQN\n",
    "from collections import namedtuple, deque   \n",
    "\"\"\"\n",
    "for t in count():\n",
    "    ... # t becomes 0, 1, 2, ... until break\n",
    "\"\"\"\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "# PyTorch library\n",
    "import torch\n",
    "# for building neural networks\n",
    "import torch.nn as nn\n",
    "# for optimization algorithms\n",
    "import torch.optim as optim\n",
    "# expose torch functions\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    # cuDNN is NVIDIA’s GPU library for deep neural network ops.\n",
    "    # speed up CNN training/inference when tensor sizes are consistent (like fixed-size images).\n",
    "    torch.backends.cudnn.benchmark = True  # auto-tune conv kernels for fixed input size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38ded1b",
   "metadata": {},
   "source": [
    "Then we make the environment.\n",
    "\n",
    "Its documentation is https://gymnasium.farama.org/environments/box2d/car_racing/.\n",
    "\n",
    "Here, we set continuous=False to make the action space discrete, because for now we only consider a discrete action space, with limited actions like \"turn left\", \"turn right\", \"accelerate\", \"brake\", and \"do nothing\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45d8b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = gym.make(\"CarRacing-v3\", continuous=False)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b34a1e",
   "metadata": {},
   "source": [
    "## Frame Preprocessing\n",
    "\n",
    "We preprocess the raw frames from the environment to make them more suitable for our neural network. The preprocessing steps include:\n",
    "\n",
    "1. **Grayscale Conversion**: Convert the RGB image to grayscale to reduce the number of input channels and focus on the essential features of the environment.\n",
    "2. **Resizing**: Resize the image to a smaller resolution (e.g., 84x84) to reduce the computational load while preserving important information.\n",
    "3. **Normalization**: Normalize the pixel values to the range [0, 1] to improve the training stability of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f9f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert RGB 96x96x3 frame to grayscale 84x84, normalized to [0,1].\"\"\"\n",
    "    # Grayscale via luminance weights\n",
    "    gray = np.dot(frame[:, :, :3], [0.2989, 0.5870, 0.1140])\n",
    "    # Crop bottom 12 rows (status bar) -> 84x96, then resize to 84x84\n",
    "    gray = gray[:84, 6:90]  # crop to 84x84 directly\n",
    "    return gray.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fd1607",
   "metadata": {},
   "source": [
    "We also create a FrameStack wrapper to stack the last 4 frames together, which allows the agent to capture temporal information and better understand the dynamics of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fac9051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameStack:\n",
    "    \"\"\"Maintains a stack of the last N preprocessed frames.\"\"\"\n",
    "    def __init__(self, n_frames: int = 4):\n",
    "        self.n_frames = n_frames\n",
    "        self.frames = deque(maxlen=n_frames)\n",
    "\n",
    "    def reset(self, frame: np.ndarray):\n",
    "        processed = preprocess_frame(frame)\n",
    "        for _ in range(self.n_frames):\n",
    "            self.frames.append(processed)\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, frame: np.ndarray):\n",
    "        self.frames.append(preprocess_frame(frame))\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self) -> np.ndarray:\n",
    "        # Returns shape (n_frames, 84, 84)\n",
    "        return np.array(self.frames, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b471d7",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "We implement a replay buffer to store the agent's experiences during training. The replay buffer allows us to sample random mini-batches of experiences for training the neural network, which helps to break the correlation between consecutive samples and improve the stability of the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3df2b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2d6c0f",
   "metadata": {},
   "source": [
    "## DQN Architecture\n",
    "\n",
    "For this implementation, we use a combination of CNN and MLP. CNN layers are used to extract features from frames, while MLP layers are used to output Q-values for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca74995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDQN(nn.Module):\n",
    "    \"\"\"Classic Atari-style CNN DQN: conv layers + fully connected layers.\"\"\"\n",
    "    def __init__(self, n_frames: int, n_actions: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(n_frames, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Compute flattened size: input 4x84x84\n",
    "        # After conv1: 32 x 20 x 20\n",
    "        # After conv2: 64 x 9 x 9\n",
    "        # After conv3: 64 x 7 x 7 = 3136\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # final output is (batch_size, n_actions)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd0cb01",
   "metadata": {},
   "source": [
    "Here, the input is:\n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{R}^{B \\times 4 \\times 84 \\times 84}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $B$ is the batch size\n",
    "- $4$ is the number of stacked frames from the FrameStack wrapper\n",
    "- $84 \\times 84$ is the resolution of each preprocessed frame\n",
    "\n",
    "### `nn.Sequential`\n",
    "\n",
    "Then, `nn.Sequential` is used to build the layers of the DQN. Typically it just applies the layers in sequence. For example:\n",
    "\n",
    "```python\n",
    "self.conv = nn.Sequential(\n",
    "    Layer1, \n",
    "    Layer2,\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "means:\n",
    "\n",
    "$$\n",
    "x_1 = \\text{Layer1}(x) \\\\\n",
    "x_2 = \\text{Layer2}(x_1) \\\\\n",
    "\\ldots\n",
    "$$\n",
    "\n",
    "### `nn.Conv2d`\n",
    "\n",
    "`nn.Conv2d(in_channels, out_channels, kernel_size, stride)` performs a 2D convolution.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$Output = Conv2D(Input, Kernel)+Bias$\n",
    "\n",
    "Where:\n",
    "- `in_channels` is the number of input feature maps (e.g., 4 for the 4 stacked frames)\n",
    "- `out_channels` is the number of output feature maps (i.e., the number of filters)\n",
    "- `kernel_size` is the size of the convolutional kernel (e.g., 8 means an 8x8 kernel)\n",
    "- `stride` is the step size for moving the kernel\n",
    "\n",
    "### How output size is computed\n",
    "\n",
    "For a 2D convolution:\n",
    "\n",
    "If input size is $(H_{in}, W_{in})$.\n",
    "\n",
    "Then output height is:\n",
    "\n",
    "$$\n",
    "H_{out} = \\left\\lfloor \\frac{H_{in} - \\text{kernel\\_size}}{\\text{stride}} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "And output width is:\n",
    "\n",
    "$$\n",
    "W_{out} = \\left\\lfloor \\frac{W_{in} - \\text{kernel\\_size}}{\\text{stride}} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "### The Actual Computation\n",
    "\n",
    "Input $B \\times 4 \\times 84 \\times 84$.\n",
    "\n",
    "In Conv1, `nn.Conv2d(4, 32, kernel_size=8, stride=4)`:\n",
    "\n",
    "$$\n",
    "H_{out} = \\left\\lfloor \\frac{84 - 8}{4} \\right\\rfloor + 1 = 20 \\\\\n",
    "W_{out} = \\left\\lfloor \\frac{84 - 8}{4} \\right\\rfloor + 1 = 20\n",
    "$$\n",
    "\n",
    "So the output of Conv1 is $B \\times 32 \\times 20 \\times 20$, where 32 is the number of filters.\n",
    "\n",
    "The following calculation is similar.\n",
    "\n",
    "\n",
    "### What does `x.view(x.size(0), -1)` do?\n",
    "\n",
    "Now in the end we have `x` with shape $B \\times 64 \\times 7 \\times 7$ after the convolutional layers.\n",
    "\n",
    "Then:\n",
    "- `x.size(0)` means the batch size $B$.\n",
    "- `-1` means we want to flatten the remaining dimensions (64, 7, 7) into a single dimension.\n",
    "\n",
    "So `x.view(x.size(0), -1)` reshapes `x` to have shape $B \\times (64 \\cdot 7 \\cdot 7)$, which is $B \\times 3136$.\n",
    "\n",
    "But how is this reshaped? It is reshaped in a row-major order (also known as C-style order). This means that the last dimension changes the fastest, and the first dimension changes the slowest. Nothing fancy.\n",
    "\n",
    "### `in_channels` and `out_channels` in `nn.Conv2d`\n",
    "\n",
    "`in_channels` is the number of input feature maps to the convolutional layer. In this concrete example, the first convolutional layer takes the 4 stacked frames as input, so `in_channels` is 4. Why do we have 4 stacked frames? Because we want the agent to capture temporal information (the movement of the car and the track), but a single frame doesn't contain that information. By stacking 4 frames together, the agent can see how the environment changes over time.\n",
    "\n",
    "`out_channels` is the number of output feature maps produced by the convolutional layer. Each filter in the convolutional layer learns to detect a specific feature in the input frames (e.g., edges, corners, etc.). By setting `out_channels` to 32, we are allowing the convolutional layer to learn 32 different features from the input frames. The more filters we have, the more complex features the layer can learn, but it also increases the computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68386ae",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here we predefine some hyperparameters.\n",
    "\n",
    "- `batch_size`: The number of samples used in one training iteration. A larger batch size can provide a more stable gradient estimate but requires more memory.\n",
    "- `gamma`: The discount factor for future rewards. It determines how much the agent values future rewards compared to immediate rewards. A value close to 1 means the agent will consider future rewards more strongly, while a value close to 0 means the agent will focus more on immediate rewards.\n",
    "- `epsilon_start`, `epsilon_end`, `epsilon_decay`: These parameters control the epsilon-greedy exploration strategy with decaying epsilon.\n",
    "- `tau`: The soft update parameter for updating the target network. A smaller `tau` means the target network updates more slowly, which can help stabilize training.\n",
    "- `learning_rate`: The learning rate for the optimizer, which controls how much the model's weights are updated during training. A smaller learning rate can lead to more stable training but may require more iterations to converge, while a larger learning rate can speed up training but may cause instability.\n",
    "- `n_frames`: The number of frames to stack together in the FrameStack wrapper. This allows the agent to capture temporal information from the environment, which is crucial for understanding the dynamics of the game. In this case, we set it to 4, meaning we will stack the last 4 frames together as input to the neural network.\n",
    "- `memory_size`: The maximum number of experiences that can be stored in the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a981987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 50000       # decay over steps (not episodes)\n",
    "TAU = 0.005             # soft target update rate\n",
    "LR = 1e-4\n",
    "N_FRAMES = 4\n",
    "MEMORY_SIZE = 50000\n",
    "OPTIMIZE_EVERY = 4      # optimize every N env steps\n",
    "PLOT_EVERY = 10         # plot every N episodes\n",
    "\n",
    "# BASE_DIR = os.path.dirname(__file__)\n",
    "RESULTS_DIR = os.path.join(\"./results\", \"car_racing\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "CHECKPOINT_PATH = os.path.join(RESULTS_DIR, \"cnn_dqn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b00746",
   "metadata": {},
   "source": [
    "## Setup for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102f602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/anaconda3/envs/rl_env/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "env = make_env()\n",
    "n_actions = env.action_space.n  # 5 discrete actions\n",
    "frame_stack = FrameStack(N_FRAMES)\n",
    "\n",
    "policy_net = CNNDQN(N_FRAMES, n_actions).to(device)\n",
    "target_net = CNNDQN(N_FRAMES, n_actions).to(device)\n",
    "\n",
    "# `amsgrad=True` can help with convergence and stability.\n",
    "# we will skip the details here.\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(MEMORY_SIZE)\n",
    "steps_done = 0\n",
    "episode_durations = []\n",
    "episode_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9187da9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the target network with the same weights as the policy network\n",
    "target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a13bb467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an action using epsilon-greedy strategy\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    # decay epsilon over time\n",
    "    # here, the threshold follows a `exponential decay` curve.\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1.0 * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if random.random() > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # policy_net(state) returns a shape of (batch_size, n_actions),\n",
    "            # so argmax(dim=1) gives the index of the max action for each batch element,\n",
    "            # because dim=1 corresponds to the action dimension.\n",
    "            # so it returns a tensor of shape (batch_size,) containing the indices of the best action for each batch element.\n",
    "            # in this particular case, batch_size=1, since we have only one state.\n",
    "            # But still, it is viewed as an array (1,), which doesn't suit the `env.step()`, so we will reshape it:\n",
    "            # `view(1, 1)` reshape it into a (1, 1) tensor. \n",
    "            return policy_net(state).argmax(dim=1).view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc03cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we train the model. Introduce a optimize_model() function to perform a single optimization step on the policy network.\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Did a tanspose here: transpose the batch\n",
    "    # from a sequence into a batch, separated by each field of the named tuple.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "        device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)    # (Batch, Frames, H, W)\n",
    "    action_batch = torch.cat(batch.action)  # (Batch, 1)\n",
    "    reward_batch = torch.cat(batch.reward)  # (Batch, 1)\n",
    "\n",
    "    # `policy_net(state_batch)` returns `(Batch, Action)`, but the `Action` here\n",
    "    # is not the action taken, but the predicted Q-values for all actions.\n",
    "    # `action_batch` stores the action actually taken for a given state.\n",
    "    # so `gather(1, action_batch)` look along dimension 1 (the action dimension),\n",
    "    # and picks the Q-value corresponding to the action taken in each transition.\n",
    "    \"\"\"\n",
    "    For example, suppose the policy network returns:\n",
    "\n",
    "    [[0.1, 0.5, 0.2],\n",
    "     [0.3, 0.4, 0.6]]\n",
    "\n",
    "    and the action_batch is:\n",
    "    [[1],\n",
    "     [2]]\n",
    "\n",
    "    Then `gather(1, action_batch)` will pick:\n",
    "    [[0.5],  # from the first row, pick index 1\n",
    "     [0.6]]  # from the second row, pick index 2\n",
    "    \"\"\"\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        # max(1) returns the max Q-value for each next state across all actions, and `.values` gives the actual max values (not the indices).\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # clip gradients to prevent explosion. here the limit is set to 100.\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35205292",
   "metadata": {},
   "source": [
    "## What does `batch = Transition(*zip(*transitions))` do?\n",
    "\n",
    "batch = Transition(*zip(*transitions)) is a transpose/re-batching step.\n",
    "- transitions is a list of sampled items from replay memory, each like:\n",
    "  Transition(state, action, next_state, reward)\n",
    "- So shape-wise it is like:  \n",
    "  [ (s1,a1,ns1,r1), (s2,a2,ns2,r2), ..., (sB,aB,nsB,rB) ]\n",
    "zip(*transitions) flips that into grouped fields:\n",
    "- (s1, s2, ..., sB)\n",
    "- (a1, a2, ..., aB)\n",
    "- (ns1, ns2, ..., nsB)\n",
    "- (r1, r2, ..., rB)\n",
    "Then Transition(*...) wraps those groups back into a named tuple, so you can access:\n",
    "- batch.state\n",
    "- batch.action\n",
    "- batch.next_state\n",
    "- batch.reward\n",
    "Why this is needed here:\n",
    "- The later code is field-wise and vectorized: torch.cat(batch.state), torch.cat(batch.action), mask over batch.next_state, etc.\n",
    "- Without this transform, you’d have to loop over each sampled transition and manually collect each field. This line does that cleanly in one step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b6827a",
   "metadata": {},
   "source": [
    "## What does `non_final_mask` do?\n",
    "\n",
    "`non_final_mask` exists for one reason: **some transitions end an episode**, so they have **no valid next state** to bootstrap from.\n",
    "\n",
    "In many replay-buffer implementations (including the PyTorch DQN tutorial this resembles), they store terminal transitions like this:\n",
    "\n",
    "* if the episode ended after taking action $a_t$, then `next_state = None`\n",
    "\n",
    "That `None` is the “done” signal.\n",
    "\n",
    "---\n",
    "\n",
    "### 1) What does DQN want to compute here?\n",
    "\n",
    "For each sampled transition $(s_t, a_t, r_{t+1}, s_{t+1})$, DQN builds a TD target:\n",
    "\n",
    "If the next state is non terminal:\n",
    "\n",
    "$$\n",
    "y_t = r_{t+1} + \\gamma \\max_{a'} Q_{\\text{target}}(s_{t+1}, a')\n",
    "$$\n",
    "\n",
    "If the next state is terminal (episode ended), there is no future return to add, so:\n",
    "\n",
    "$$\n",
    "y_t = r_{t+1}\n",
    "$$\n",
    "\n",
    "Meaning of each term:\n",
    "\n",
    "* $y_t$: training target for the Q-value of the chosen action at $s_t$\n",
    "* $r_{t+1}$: immediate reward after taking $a_t$ at $s_t$\n",
    "* $\\gamma$: discount factor (how much you care about the future)\n",
    "* $Q_{\\text{target}}(\\cdot)$: Q-network used for the target (here `target_net`)\n",
    "* $\\max_{a'}$: greedy value over all possible next actions\n",
    "\n",
    "So terminal transitions must **not** include the $\\gamma \\max Q(\\cdot)$ part.\n",
    "\n",
    "That is exactly what the mask implements.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) What `non_final_mask` is doing\n",
    "\n",
    "```python\n",
    "non_final_mask = torch.tensor(\n",
    "    tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "    device=device, dtype=torch.bool)\n",
    "```\n",
    "\n",
    "This creates a boolean tensor of length `BATCH_SIZE`.\n",
    "\n",
    "* `True` means: this transition has a real next state, so it is **non terminal**\n",
    "* `False` means: `next_state is None`, so it is **terminal**\n",
    "\n",
    "Example: if the batch has 5 transitions and two are terminal, you might get:\n",
    "\n",
    "* `non_final_mask = [True, False, True, True, False]`\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Why `non_final_next_states` exists\n",
    "\n",
    "```python\n",
    "non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "```\n",
    "\n",
    "You cannot do `torch.cat` on a list that contains `None`.\n",
    "\n",
    "So they extract only the actual tensors and concatenate them into one tensor, so they can run them through `target_net` in one shot.\n",
    "\n",
    "---\n",
    "\n",
    "### 4) How the mask is used to fill `next_state_values`\n",
    "\n",
    "```python\n",
    "next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "with torch.no_grad():\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "```\n",
    "\n",
    "Step by step:\n",
    "\n",
    "1. Start with all zeros:\n",
    "\n",
    "* `next_state_values[i] = 0` for every transition $i$\n",
    "\n",
    "This already matches the terminal rule:\n",
    "\n",
    "* if transition $i$ is terminal, we want the bootstrapped term to be 0\n",
    "\n",
    "2. For non terminal transitions only, compute:\n",
    "\n",
    "$$\n",
    "\\max_{a'} Q_{\\text{target}}(s_{t+1}, a')\n",
    "$$\n",
    "\n",
    "and write those values back into the right positions using the mask:\n",
    "\n",
    "* `next_state_values[non_final_mask] = ...`\n",
    "\n",
    "So after this:\n",
    "\n",
    "* terminal transitions keep value 0\n",
    "* non terminal transitions get their max-Q bootstrap value\n",
    "\n",
    "This is a clean way to implement the piecewise definition of $y_t$.\n",
    "\n",
    "---\n",
    "\n",
    "### 5) Where this shows up in the target in your code\n",
    "\n",
    "```python\n",
    "expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "```\n",
    "\n",
    "This is exactly:\n",
    "\n",
    "$$\n",
    "y_t = r_{t+1} + \\gamma \\cdot \\text{next_state_value}\n",
    "$$\n",
    "\n",
    "and because terminal transitions have `next_state_value = 0`, you automatically get:\n",
    "\n",
    "$$\n",
    "y_t = r_{t+1}\n",
    "$$\n",
    "\n",
    "for terminal transitions.\n",
    "\n",
    "---\n",
    "\n",
    "### 6) Why you did not “see it in the original paper”\n",
    "\n",
    "The original DQN math usually writes the target with an implicit “if terminal then no bootstrap” condition, often described in words rather than showing an explicit boolean mask in pseudocode.\n",
    "\n",
    "In code, you must handle it concretely, and a mask is the standard vectorized way.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faec6dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally the whole training loop:\n",
    "\n",
    "skip_training = os.path.exists(CHECKPOINT_PATH) and not args.load\n",
    "\n",
    "if args.episodes:\n",
    "    num_episodes = args.episodes\n",
    "elif torch.cuda.is_available():\n",
    "    print(\"Using GPU for training.\")\n",
    "    num_episodes = 500\n",
    "else:\n",
    "    print(\"Using CPU for training.\")\n",
    "    num_episodes = 30\n",
    "\n",
    "if not skip_training:\n",
    "    for i_episode in tqdm(range(num_episodes), desc=\"Training\"):\n",
    "        obs, info = env.reset()\n",
    "        state = frame_stack.reset(obs)\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        ep_reward = 0.0\n",
    "        for t in count():\n",
    "            action = select_action(state)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "            ep_reward += reward\n",
    "            reward_tensor = torch.tensor([reward], device=device, dtype=torch.float32)\n",
    "\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = frame_stack.step(next_obs)\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            memory.push(state, action, next_state, reward_tensor)\n",
    "            state = next_state\n",
    "\n",
    "            # Optimize and update target net every N steps\n",
    "            if t % OPTIMIZE_EVERY == 0:\n",
    "                optimize_model()\n",
    "\n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key] * TAU + \\\n",
    "                                                 target_net_state_dict[key] * (1 - TAU)\n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            if done:\n",
    "                episode_durations.append(t + 1)\n",
    "                episode_rewards.append(ep_reward)\n",
    "                if i_episode % PLOT_EVERY == 0:\n",
    "                    plot_training_live()\n",
    "                break\n",
    "\n",
    "    print('Training complete')\n",
    "\n",
    "    # Save checkpoint with training metrics\n",
    "    torch.save({\n",
    "        \"policy_net\": policy_net.state_dict(),\n",
    "        \"target_net\": target_net.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"steps_done\": steps_done,\n",
    "        \"episode_durations\": episode_durations,\n",
    "        \"episode_rewards\": episode_rewards,\n",
    "    }, CHECKPOINT_PATH)\n",
    "    print(f\"Checkpoint saved to {CHECKPOINT_PATH}\")\n",
    "\n",
    "# Save final chart (always regenerated)\n",
    "if episode_rewards:\n",
    "    print(\"Evaluating random baseline (50 episodes)...\")\n",
    "    rand_dur, rand_rew = evaluate_random(num_episodes=50)\n",
    "    print(f\"  Random baseline => duration: {rand_dur:.0f}, reward: {rand_rew:.1f}\")\n",
    "    chart_path = os.path.join(RESULTS_DIR, \"cnn_dqn.png\")\n",
    "    plot_final_chart(episode_rewards, rand_rew, chart_path)\n",
    "else:\n",
    "    print(\"No training metrics available — cannot generate charts.\")\n",
    "\n",
    "env.close()\n",
    "print(f\"Done! All results saved to {RESULTS_DIR}\")\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
